{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY_TlVHQL99q",
        "outputId": "0ee971cd-e016-4401-a166-000d590535dc"
      },
      "outputs": [],
      "source": [
        "# Installing Libraries\n",
        "# !pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbvWkWuAMD3r",
        "outputId": "c161f360-19b9-44e8-fb3d-6c71913bb146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Dzope\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
          ]
        }
      ],
      "source": [
        "#Imports\n",
        "import nltk\n",
        "#nltk.download('punkt')\n",
        "import os\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eP_Pgfw_MK-6"
      },
      "outputs": [],
      "source": [
        "#Loading the intents data\n",
        "with open(\"intents.json\") as file:\n",
        "\tdata = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RIet9zfKMYLj"
      },
      "outputs": [],
      "source": [
        "#Initializing empty lists\n",
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ocNwF1E3MaVM"
      },
      "outputs": [],
      "source": [
        "#Looping through our data\n",
        "for intent in data['intents']:\n",
        "\tfor pattern in intent['patterns']:\n",
        "\t\tpattern = pattern.lower()\n",
        "    #Creating a list of words\n",
        "\t\twrds = nltk.word_tokenize(pattern)\n",
        "\t\twords.extend(wrds)\n",
        "\t\tdocs_x.append(wrds)\n",
        "\t\tdocs_y.append(intent['tag'])\n",
        "\n",
        "\tif intent['tag'] not in labels:\n",
        "\t  labels.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gOJs1rmvMhvO"
      },
      "outputs": [],
      "source": [
        "#Pre-Processing the given data\n",
        "stemmer = LancasterStemmer()\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "labels = sorted(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FvM4MjVIMopz"
      },
      "outputs": [],
      "source": [
        "training = []\n",
        "output = []\n",
        "\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "for x,doc in enumerate(docs_x):\n",
        "\tbag = []\n",
        "\twrds = [stemmer.stem(w) for w in doc]\n",
        "\tfor w in words:\n",
        "\t\tif w in wrds:\n",
        "\t\t\tbag.append(1)\n",
        "\t\telse:\n",
        "\t\t\tbag.append(0)\n",
        "\toutput_row = out_empty[:]\n",
        "\toutput_row[labels.index(docs_y[x])] = 1\n",
        "\ttraining.append(bag)\n",
        "\toutput.append(output_row)\n",
        "#Converting training data into NumPy arrays\n",
        "training = np.array(training)\n",
        "output = np.array(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CBtJ8v6qMwYH"
      },
      "outputs": [],
      "source": [
        "#Saving data to disk\n",
        "with open(\"data.pickle\",\"wb\") as f:\n",
        "\tpickle.dump((words, labels, training, output),f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hze8v8v7MyMM",
        "outputId": "25403d4f-77a2-4a0f-ddf2-31e12037babe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.11812\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 200 | loss: 0.11812 - acc: 0.9999 -- iter: 40/43\n",
            "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.86831\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 200 | loss: 0.86831 - acc: 0.9124 -- iter: 43/43\n",
            "--\n",
            "INFO:tensorflow:d:\\Alwi\\TUGAS KULIAH SEMESTER 8\\chatbot_yaelahwi\\model\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ],
      "source": [
        "#Generating and Saving ML Model\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape = [None, len(training[0])])\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,8)\n",
        "net = tflearn.fully_connected(net,len(output[0]), activation = \"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(training, output, n_epoch = 200, batch_size = 8, show_metric = True)\n",
        "model.save(\"model.tflearn\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Movie Recommender Chatbot.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "8e6d79204f2cef2cf38925ef666cbb328506b5aa027b7df21064ee6742f89f9c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
